{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization API\n",
    "## Call the method \"text_summarization(title, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SummaryTool(object):\n",
    "\n",
    "    # Naive method for splitting a text into sentences\n",
    "    def split_content_to_sentences(self, content):\n",
    "        content = content.replace(\"\\n\", \". \")\n",
    "        return content.split(\". \")\n",
    "\n",
    "    # Naive method for splitting a text into paragraphs\n",
    "    def split_content_to_paragraphs(self, content):\n",
    "        return content.split(\"\\n\\n\")\n",
    "\n",
    "    # Caculate the intersection between 2 sentences\n",
    "    def sentences_intersection(self, sent1, sent2):\n",
    "\n",
    "        # split the sentence into words/tokens\n",
    "        s1 = set(sent1.split(\" \"))\n",
    "        s2 = set(sent2.split(\" \"))\n",
    "\n",
    "        # If there is not intersection, just return 0\n",
    "        if (len(s1) + len(s2)) == 0:\n",
    "            return 0\n",
    "\n",
    "        # We normalize the result by the average number of words\n",
    "        return len(s1.intersection(s2)) / ((len(s1) + len(s2)) / 2)\n",
    "\n",
    "    # Format a sentence - remove all non-alphbetic chars from the sentence\n",
    "    # We'll use the formatted sentence as a key in our sentences dictionary\n",
    "    def format_sentence(self, sentence):\n",
    "        sentence = re.sub(r'\\W+', '', sentence)\n",
    "        return sentence\n",
    "\n",
    "    # Convert the content into a dictionary <K, V>\n",
    "    # k = The formatted sentence\n",
    "    # V = The rank of the sentence\n",
    "    def get_sentence_ranks(self, content):\n",
    "\n",
    "        # Split the content into sentences\n",
    "        sentences = self.split_content_to_sentences(content)\n",
    "\n",
    "        # Calculate the intersection of every two sentences\n",
    "        n = len(sentences)\n",
    "        values = [[0 for x in range(n)] for x in range(n)]\n",
    "        for i in range(0, n):\n",
    "            for j in range(0, n):\n",
    "                values[i][j] = self.sentences_intersection(sentences[i], sentences[j])\n",
    "\n",
    "        # Build the sentences dictionary\n",
    "        # The score of a sentences is the sum of all its intersection\n",
    "        sentences_dic = {}\n",
    "        for i in range(0, n):\n",
    "            score = 0\n",
    "            for j in range(0, n):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                score += values[i][j]\n",
    "            sentences_dic[self.format_sentence(sentences[i])] = score\n",
    "        return sentences_dic\n",
    "\n",
    "    # Return the best sentence in a paragraph\n",
    "    def get_best_sentence(self, paragraph, sentences_dic):\n",
    "\n",
    "        # Split the paragraph into sentences\n",
    "        sentences = self.split_content_to_sentences(paragraph)\n",
    "\n",
    "        # Ignore short paragraphs\n",
    "        if len(sentences) < 2:\n",
    "            return \"\"\n",
    "\n",
    "        # Get the best sentence according to the sentences dictionary\n",
    "        best_sentence = \"\"\n",
    "        max_value = 0\n",
    "        for s in sentences:\n",
    "            strip_s = self.format_sentence(s)\n",
    "            if strip_s:\n",
    "                if sentences_dic[strip_s] > max_value:\n",
    "                    max_value = sentences_dic[strip_s]\n",
    "                    best_sentence = s\n",
    "\n",
    "        return best_sentence\n",
    "\n",
    "    # Build the summary\n",
    "    def get_summary(self, title, content, sentences_dic):\n",
    "\n",
    "        # Split the content into paragraphs\n",
    "        paragraphs = self.split_content_to_paragraphs(content)\n",
    "\n",
    "        # Add the title\n",
    "        summary = []\n",
    "        summary.append(title.strip())\n",
    "        summary.append(\"\")\n",
    "\n",
    "        # Add the best sentence from each paragraph\n",
    "        for p in paragraphs:\n",
    "            sentence = self.get_best_sentence(p, sentences_dic).strip()\n",
    "            if sentence:\n",
    "                summary.append(sentence)\n",
    "\n",
    "        return (\"\\n\").join(summary)\n",
    "\n",
    "\n",
    "def text_summarization(title, content):\n",
    "    # Create a SummaryTool object\n",
    "    st = SummaryTool()\n",
    "\n",
    "    # Build the sentences dictionary\n",
    "    sentences_dic = st.get_sentence_ranks(content)\n",
    "\n",
    "    # Build the summary with the sentences dictionary\n",
    "    summary = st.get_summary(title, content, sentences_dic)\n",
    "\n",
    "    # Print the summary\n",
    "    #print (summary)\n",
    "    return summary\n",
    "    \n",
    "    # Print the ratio between the summary length and the original length\n",
    "    #print (\"\")\n",
    "    #print (\"Original Length %s\" % (len(title) + len(content)))\n",
    "    #print (\"Summary Length %s\" % len(summary))\n",
    "    #print (\"Summary Ratio: %s\" % (100 - (100 * (len(summary) / (len(title) + len(content))))))\n",
    "\n",
    "    \n",
    "title = 'doc1'\n",
    "summary = \"\"\"Take a low budget; inexperienced actors doubling as production staff?? as well as limited facilities??and you can't expect much more than \"Time Chasers\" gives you; but you can absolutely expect a lot less. This film represents a bunch of good natured friends and neighbors coming together to collaborate on an interesting project. If your cousin had been one of those involved; you would probably think to yourself; \"ok; this movie is terrible... but a really good effort.\" For all the poorly delivered dialog and ham-fisted editing; \"Time Chasers\" has great scope and ambition... and one can imagine it was necessary to shoot every scene in only one or two takes. So; I'm suggesting people cut \"Time Chasers\" some slack before they cut in the jugular. That said; I'm not sure I can ever forgive the pseudo-old lady from the grocery store for the worst delivery every wrenched from the jaws of a problematic script.\n",
    "Having seen this movie more often than all others; and one of those you will never forget the theater in which you saw it the first time; it undoubtedly is Doris Day's best. I am one of many who also feel she deserved an academy award for her portrayal of Ruth Etting. With her tremendous voice; and the acting skills she demonstrated in this role; one can only imagine what she would have done had she been one of the MGM stars; when their musicals were known as the best. I understand she came within a hair of being cast in South Pacific; but as has been rumored; it was either Marty Melcher's insistence on a much higher salary; or a clash between Doris or Marty with the director. From reading her biography; it is very apparent that she wasn't one of your typical Hollywood stars. With these typical stars and directors; as well as most everyone else associated with the AMPAS being those who decide who be will be nominated; as well as ultimately winning; Ms. Day's choice of living a less typical Hollywood life style may have had some bearing on her not being nominated for Love Me or Leave Me. I definitely believe this is the reason she has never been given a Lifetime Achievement Award from the Academy of Motion Picture Arts and Sciences. Let us hope this award will be made before this gracious and lovely lady is unable to make an appearance there.\n",
    "\"\"\"\n",
    "\n",
    "result = text_summarization(title, summary)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Sentiment Analysis Model (using Random Forest Algorithm)\n",
    "## Saved File name: \"MT_Trained_Sentiment_Model_Imdb.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# inputs\n",
    "_train_data_csv = 'train_imdb.csv'\n",
    "filename = 'MT_Trained_Sentiment_Model_Imdb.sav'\n",
    "\n",
    "train_data = pd.read_csv(_train_data_csv)\n",
    "\n",
    "def drop_features(features,data):\n",
    "    data.drop(features,inplace=True,axis=1)\n",
    "    \n",
    "def process_data(i_data):\n",
    "    return \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",i_data.lower()).split())\n",
    "\n",
    "train_data['processed_review'] = train_data['review'].apply(process_data)\n",
    "\n",
    "drop_features(['review'],train_data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data[\"processed_review\"],train_data[\"label\"], test_size = 0.2, random_state = 42)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n",
    "\n",
    "x_train_counts = count_vect.fit_transform(x_train)\n",
    "x_train_tfidf = transformer.fit_transform(x_train_counts)\n",
    "\n",
    "x_test_counts = count_vect.transform(x_test)\n",
    "x_test_tfidf = transformer.transform(x_test_counts)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=200)\n",
    "model.fit(x_train_tfidf,y_train)\n",
    "\n",
    "#predictions = model.predict(x_test_tfidf)\n",
    "\n",
    "#from sklearn.metrics import confusion_matrix,f1_score\n",
    "#confusion_matrix(y_test,predictions)\n",
    "\n",
    "#train_counts = count_vect.fit_transform(train_data['processed_review'])\n",
    "#train_tfidf = transformer.fit_transform(train_counts)\n",
    "\n",
    "#final_model = model.fit(train_tfidf,train_data['label'])\n",
    "\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "def drop_features(features,data):\n",
    "    data.drop(features,inplace=True,axis=1)\n",
    "    \n",
    "def process_data(i_data):\n",
    "    return \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",i_data.lower()).split())\n",
    "\n",
    "from textblob import TextBlob\n",
    "def model_predict(test_data):\n",
    "    p = []\n",
    "    for x in test_data['review']:\n",
    "        blob = TextBlob(x)\n",
    "        p.append( \"0\" if blob.sentiment.polarity < 0 else \"1\")\n",
    "    return p\n",
    "\n",
    "def calculate_sentiment(test_data):\n",
    "    _train_data_csv = 'train_imdb.csv'\n",
    "    train_data = pd.read_csv(_train_data_csv)\n",
    "    train_data['processed_review'] = train_data['review'].apply(process_data)\n",
    "\n",
    "    drop_features(['review'],train_data)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_data[\"processed_review\"],train_data[\"label\"], test_size = 0.2, random_state = 42)\n",
    "    count_vect = CountVectorizer(stop_words='english')\n",
    "    transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n",
    "    x_train_counts = count_vect.fit_transform(x_train)\n",
    "    x_train_tfidf = transformer.fit_transform(x_train_counts)\n",
    "    x_test_counts = count_vect.transform(x_test)\n",
    "    x_test_tfidf = transformer.transform(x_test_counts)\n",
    "\n",
    "\n",
    "    loaded_model = pickle.load(open('MT_Trained_Sentiment_Model_Imdb.sav', 'rb'))\n",
    "    #test_data = pd.read_csv('Testing_data_MT.csv')\n",
    "    test_data['processed_review'] = test_data['review'].apply(process_data)\n",
    "    test_counts = count_vect.transform(test_data['processed_review'])\n",
    "    test_tfidf = transformer.transform(test_counts)\n",
    "\n",
    "    predictions = loaded_model.predict(test_tfidf)\n",
    "    #predictions = model_predict(test_data)\n",
    "    test_data['label'] = predictions\n",
    "    final_result = pd.DataFrame({'id':test_data['id'],'review':test_data['processed_review'],'label':predictions})\n",
    "    final_result.to_csv('output.csv',index=False)\n",
    "    return final_result\n",
    "#final_result\n",
    "\n",
    "test_data = pd.read_csv('test_test_imdb_utf8.csv')\n",
    "result2 = calculate_sentiment(test_data)\n",
    "#print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My REST Service API modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8081/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [14/Nov/2018 19:20:01] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:21:13] \"\u001b[37mPOST /sentiment HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:22:39] \"\u001b[37mOPTIONS /sentiment HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:22:57] \"\u001b[37mPOST /sentiment HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:23:22] \"\u001b[37mOPTIONS /detectTopic HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:23:24] \"\u001b[37mPOST /detectTopic HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:23:24] \"\u001b[37mOPTIONS /generateCluster HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:23:24] \"\u001b[37mPOST /generateCluster HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:23:43] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:23:43] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:23:51] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:23:51] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:23:55] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:24:03] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:24:03] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:24:17] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:24:17] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:24:21] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:24:30] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:24:30] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:25:26] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:25:26] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:25:35] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:25:35] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:28] \"\u001b[37mOPTIONS /detectTopic HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:28] \"\u001b[37mPOST /detectTopic HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:28] \"\u001b[37mOPTIONS /generateCluster HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:28] \"\u001b[37mPOST /generateCluster HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:30] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:30] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:31] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:35] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:35] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:38] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:40] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:42] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:42] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:44] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:48] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:48] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:50] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:54] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:54] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:27:56] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:28:00] \"\u001b[37mOPTIONS /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:28:00] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:58:45] \"\u001b[37mOPTIONS /detectTopic HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:58:45] \"\u001b[37mPOST /detectTopic HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:58:45] \"\u001b[37mOPTIONS /generateCluster HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 19:58:45] \"\u001b[37mPOST /generateCluster HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [14/Nov/2018 20:23:44] \"\u001b[37mPOST /summary HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from pandas import DataFrame\n",
    "\n",
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "def drop_features(features,data):\n",
    "    data.drop(features,inplace=True,axis=1)\n",
    "    \n",
    "def process_data(i_data):\n",
    "    return \" \".join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])\", \" \",i_data.lower()).split())\n",
    "\n",
    "from textblob import TextBlob\n",
    "def model_predict(test_data):\n",
    "    p = []\n",
    "    for x in test_data['review']:\n",
    "        blob = TextBlob(x)\n",
    "        p.append( \"0\" if blob.sentiment.polarity < 0 else \"1\")\n",
    "    return p\n",
    "\n",
    "#@app.route('/sentiment/<string:test_csv>')\n",
    "@app.route('/sentiment', methods=['POST'])\n",
    "def calculate_sentiment():\n",
    "    #test_data = pd.read_csv(test_csv)\n",
    "    json_data = request.get_json()\n",
    "    \n",
    "    test_data = DataFrame(json_data)\n",
    "    \n",
    "    test_data = test_data.rename(columns={'answerid': 'id', 'atxt': 'review'})\n",
    "    \n",
    "    _train_data_csv = 'train_imdb.csv'\n",
    "    train_data = pd.read_csv(_train_data_csv)\n",
    "    train_data['processed_review'] = train_data['review'].apply(process_data)\n",
    "\n",
    "    drop_features(['review'],train_data)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_data[\"processed_review\"],train_data[\"label\"], test_size = 0.2, random_state = 42)\n",
    "    count_vect = CountVectorizer(stop_words='english')\n",
    "    transformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n",
    "    x_train_counts = count_vect.fit_transform(x_train)\n",
    "    x_train_tfidf = transformer.fit_transform(x_train_counts)\n",
    "    x_test_counts = count_vect.transform(x_test)\n",
    "    x_test_tfidf = transformer.transform(x_test_counts)\n",
    "\n",
    "\n",
    "    loaded_model = pickle.load(open('MT_Trained_Sentiment_Model_Imdb.sav', 'rb'))\n",
    "    #test_data = pd.read_csv('Testing_data_MT.csv')\n",
    "    test_data['processed_review'] = test_data['review'].apply(process_data)\n",
    "    test_counts = count_vect.transform(test_data['processed_review'])\n",
    "    test_tfidf = transformer.transform(test_counts)\n",
    "\n",
    "    predictions = loaded_model.predict(test_tfidf)\n",
    "    predictions = model_predict(test_data)\n",
    "    \n",
    "    test_data['polarity'] = predictions\n",
    "    \n",
    "    final_result = test_data.rename(columns={'id': 'answerid', 'review': 'atxt'})\n",
    "    #final_result = pd.DataFrame({'answerid':test_data['id'],'polarity':predictions})\n",
    "    result_json = final_result.to_json(orient='records')\n",
    "    \n",
    "    return jsonify(result_json)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def testing():\n",
    "     return 'My M.Tech REST API Service is active...'\n",
    "    \n",
    "    \n",
    "class SummaryTool(object):\n",
    "\n",
    "    # Naive method for splitting a text into sentences\n",
    "    def split_content_to_sentences(self, content):\n",
    "        content = content.replace(\"\\n\", \". \")\n",
    "        return content.split(\". \")\n",
    "\n",
    "    # Naive method for splitting a text into paragraphs\n",
    "    def split_content_to_paragraphs(self, content):\n",
    "        return content.split(\"\\n\\n\")\n",
    "\n",
    "    # Caculate the intersection between 2 sentences\n",
    "    def sentences_intersection(self, sent1, sent2):\n",
    "\n",
    "        # split the sentence into words/tokens\n",
    "        s1 = set(sent1.split(\" \"))\n",
    "        s2 = set(sent2.split(\" \"))\n",
    "\n",
    "        # If there is not intersection, just return 0\n",
    "        if (len(s1) + len(s2)) == 0:\n",
    "            return 0\n",
    "\n",
    "        # We normalize the result by the average number of words\n",
    "        return len(s1.intersection(s2)) / ((len(s1) + len(s2)) / 2)\n",
    "\n",
    "    # Format a sentence - remove all non-alphbetic chars from the sentence\n",
    "    # We'll use the formatted sentence as a key in our sentences dictionary\n",
    "    def format_sentence(self, sentence):\n",
    "        sentence = re.sub(r'\\W+', '', sentence)\n",
    "        return sentence\n",
    "\n",
    "    # Convert the content into a dictionary <K, V>\n",
    "    # k = The formatted sentence\n",
    "    # V = The rank of the sentence\n",
    "    def get_sentence_ranks(self, content):\n",
    "\n",
    "        # Split the content into sentences\n",
    "        sentences = self.split_content_to_sentences(content)\n",
    "\n",
    "        # Calculate the intersection of every two sentences\n",
    "        n = len(sentences)\n",
    "        values = [[0 for x in range(n)] for x in range(n)]\n",
    "        for i in range(0, n):\n",
    "            for j in range(0, n):\n",
    "                values[i][j] = self.sentences_intersection(sentences[i], sentences[j])\n",
    "\n",
    "        # Build the sentences dictionary\n",
    "        # The score of a sentences is the sum of all its intersection\n",
    "        sentences_dic = {}\n",
    "        for i in range(0, n):\n",
    "            score = 0\n",
    "            for j in range(0, n):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                score += values[i][j]\n",
    "            sentences_dic[self.format_sentence(sentences[i])] = score\n",
    "        return sentences_dic\n",
    "\n",
    "    # Return the best sentence in a paragraph\n",
    "    def get_best_sentence(self, paragraph, sentences_dic):\n",
    "\n",
    "        # Split the paragraph into sentences\n",
    "        sentences = self.split_content_to_sentences(paragraph)\n",
    "\n",
    "        # Ignore short paragraphs\n",
    "        if len(sentences) < 2:\n",
    "            return \"\"\n",
    "\n",
    "        # Get the best sentence according to the sentences dictionary\n",
    "        best_sentence = \"\"\n",
    "        max_value = 0\n",
    "        for s in sentences:\n",
    "            strip_s = self.format_sentence(s)\n",
    "            if strip_s:\n",
    "                if sentences_dic[strip_s] > max_value:\n",
    "                    max_value = sentences_dic[strip_s]\n",
    "                    best_sentence = s\n",
    "\n",
    "        return best_sentence\n",
    "\n",
    "    # Build the summary\n",
    "    def get_summary(self, title, content, sentences_dic):\n",
    "\n",
    "        # Split the content into paragraphs\n",
    "        paragraphs = self.split_content_to_paragraphs(content)\n",
    "\n",
    "        # Add the title\n",
    "        summary = []\n",
    "        summary.append(title.strip())\n",
    "        summary.append(\"\")\n",
    "\n",
    "        # Add the best sentence from each paragraph\n",
    "        for p in paragraphs:\n",
    "            sentence = self.get_best_sentence(p, sentences_dic).strip()\n",
    "            if sentence:\n",
    "                summary.append(sentence)\n",
    "\n",
    "        return (\"\\n\").join(summary)\n",
    "\n",
    "#@app.route('/summary/<string:content>')\n",
    "@app.route('/summary', methods=['POST'])\n",
    "def text_summarization():\n",
    "    # Create a SummaryTool object\n",
    "    json_data = request.get_json()\n",
    "    #print('JSON_DATA: ',json_data)\n",
    "    input_data = DataFrame(json_data)\n",
    "        \n",
    "    \n",
    "    temp = pd.read_csv('clusters_output.csv')\n",
    "    test_data = temp[temp['Name'] == input_data.Name[0]]\n",
    "\n",
    "    content = \"\"\n",
    "    for index, row in test_data.iterrows():\n",
    "        content = content + row.Sentences\n",
    "\n",
    "    st = SummaryTool()\n",
    "    \n",
    "    #Build a sentence dictionary\n",
    "    sentences_dic = st.get_sentence_ranks(content)\n",
    "    title = \"\"\n",
    "    # Build the summary with the sentences dictionary\n",
    "    summary = st.get_summary(title, content, sentences_dic)\n",
    "    \n",
    "        \n",
    "    return jsonify(summary)\n",
    "    \n",
    "    #print(test_data)\n",
    "    #result_json = test_data.to_json(orient='records')\n",
    "    #return jsonify(result_json)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "\n",
    "@app.route('/detectTopic', methods=['POST'])\n",
    "def detect_topic():\n",
    "    \n",
    "    json_data = request.get_json()\n",
    "    input_data = DataFrame(json_data)\n",
    "    input_data = input_data.rename(columns={'answerid': 'id', 'atxt': 'review'})\n",
    "\n",
    "    df = pd.DataFrame(columns=['Name', 'Sentences', 'Summary', 'Name_Count'],dtype=int)\n",
    "    \n",
    "    for index, row in input_data.iterrows():\n",
    "        \n",
    "        blob = TextBlob(row.review)\n",
    "        for sentence in blob.sentences:\n",
    "            s_sent = str(sentence)\n",
    "            nouns = list()\n",
    "            for word, tag in sentence.tags:\n",
    "                if tag == 'NN':\n",
    "                    nouns.append(word.lemmatize())\n",
    "\n",
    "            for n in nouns:\n",
    "                n_plrl = Word(n).pluralize()\n",
    "                my_df_list = df.index[df['Name'] == n_plrl].tolist()\n",
    "                if(len(my_df_list) == 0):\n",
    "                    df.loc[len(df)] = [n_plrl, s_sent, '', 1]\n",
    "                else:\n",
    "                    i = int(my_df_list[0])\n",
    "                    df.Name_Count[i] = df.Name_Count[i] + 1\n",
    "                    df.Sentences[i] = df.Sentences[i] + '. ' +s_sent \n",
    "                    \n",
    "    result_json = df.to_json(orient='records')\n",
    "    return jsonify(result_json)\n",
    "\n",
    "\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "@app.route('/generateCluster', methods=['POST'])\n",
    "def generate_cluster():\n",
    "    \n",
    "    json_data = request.get_json()\n",
    "    df = DataFrame(json_data)\n",
    "    \n",
    "    threshold_value = 0.60\n",
    "    w, h = len(df), len(df)\n",
    "    Cosine_Matrix = [[0 for x in range(w)] for y in range(h)] \n",
    "\n",
    "    for index1, row1 in df.iterrows():\n",
    "        for index2, row2 in df.iterrows():\n",
    "            vector1 = text_to_vector(row1.Sentences)\n",
    "            vector2 = text_to_vector(row2.Sentences)\n",
    "\n",
    "            cosine = get_cosine(vector1, vector2)\n",
    "            Cosine_Matrix[index1][index2] = cosine\n",
    "\n",
    "    #cm = pd.DataFrame(Cosine_Matrix)      \n",
    "    #cm.to_csv('cosine_output.csv')\n",
    "    #cm\n",
    "    cluster_df = pd.DataFrame(columns=['Name', 'Sentences', 'Summary', 'Name_Count'],dtype=int)\n",
    "\n",
    "    for index1, row1 in df.iterrows():\n",
    "        name = row1.Name\n",
    "        sentences = row1.Sentences\n",
    "        name_count = row1.Name_Count\n",
    "        for index2, row2 in df.iterrows():\n",
    "            if index1 != index2:\n",
    "                if Cosine_Matrix[index1][index2] >= threshold_value:\n",
    "                    name = name +', '+ row2.Name\n",
    "                    sentences = sentences +'. '+ row2.Sentences\n",
    "                    name_count = name_count + 1\n",
    "\n",
    "        if name != row1.Name:\n",
    "            #data = name.split(\",\")\n",
    "            data = [x.strip() for x in name.split(',')]\n",
    "            data.sort()\n",
    "            name = ', '.join(data)\n",
    "            cluster_df.loc[len(cluster_df)] =[name, sentences, '', name_count]\n",
    "\n",
    "    # sorting by first name \n",
    "    cluster_df.sort_values(\"Name\", inplace = True) \n",
    "\n",
    "    # dropping ALL duplicte values \n",
    "    cluster_df.drop_duplicates(subset =\"Name\", keep = False, inplace = True) \n",
    "    \n",
    "    frames = [df, cluster_df]\n",
    "    combined_result = pd.concat(frames)\n",
    "    combined_result.to_csv('clusters_output.csv', encoding='utf-8')\n",
    "\n",
    "    \n",
    "    result_json = cluster_df.to_json(orient='records')\n",
    "    return jsonify(result_json)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "app.run(port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
